{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 1, Module 4*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Project: Logistic Regression\n",
    "\n",
    "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
    "\n",
    "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
    "\n",
    "## Directions\n",
    "\n",
    "The tasks for this project are the following:\n",
    "\n",
    "- **Task 1:** Import `csv` file using `wrangle` function.\n",
    "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
    "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
    "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
    "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
    "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
    "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
    "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
    "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
    "\n",
    "**Note** \n",
    "\n",
    "You should limit yourself to the following libraries:\n",
    "\n",
    "- `category_encoders`\n",
    "- `matplotlib`\n",
    "- `pandas`\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Import w/ DateTimeIndex\n",
    "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
    "                     index_col='Date')\n",
    "    \n",
    "    # Drop unrated burritos\n",
    "    df.dropna(subset=['overall'], inplace=True)\n",
    "    \n",
    "    # Derive binary classification target:\n",
    "    # We define a 'Great' burrito as having an\n",
    "    # overall rating of 4 or higher, on a 5 point scale\n",
    "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
    "    \n",
    "    # Drop high cardinality categoricals\n",
    "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood', 'Burrito', 'Reviewer'])\n",
    "    \n",
    "    # Drop columns to prevent \"leakage\"\n",
    "    df = df.drop(columns=['Rec', 'overall'])\n",
    "    \n",
    "    # Drop columns that have 0 options\n",
    "    df = df.drop(columns=['Queso'])\n",
    "    # Drop columns don't know what it is\n",
    "    df = df.drop(columns=['NonSD', 'Unreliable'])\n",
    "    \n",
    "    df.iloc[:,21:-1] = df.iloc[:,21:-1].fillna(0)\n",
    "    df.iloc[:,21:-1] = df.iloc[:,21:-1].replace(['x', 'X'], [1, 1])\n",
    "    df['Chips'] = df['Chips'].fillna(0).replace(['x', 'X', 'Yes', 'No'], [1, 1, 1, 0])\n",
    "    \n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "filepath = DATA_PATH + 'burritos/burritos.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Yelp</th>\n",
       "      <th>Google</th>\n",
       "      <th>Chips</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Hunger</th>\n",
       "      <th>Mass (g)</th>\n",
       "      <th>Density (g/mL)</th>\n",
       "      <th>Length</th>\n",
       "      <th>Circum</th>\n",
       "      <th>Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>Nopales</th>\n",
       "      <th>Lobster</th>\n",
       "      <th>Egg</th>\n",
       "      <th>Mushroom</th>\n",
       "      <th>Bacon</th>\n",
       "      <th>Sushi</th>\n",
       "      <th>Avocado</th>\n",
       "      <th>Corn</th>\n",
       "      <th>Zucchini</th>\n",
       "      <th>Great</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-18</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.49</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>3.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6.59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Yelp  Google  Chips  Cost  Hunger  Mass (g)  Density (g/mL)  \\\n",
       "Date                                                                      \n",
       "2016-01-18   3.5     4.2      0  6.49     3.0       NaN             NaN   \n",
       "2016-01-24   3.5     3.3      0  5.45     3.5       NaN             NaN   \n",
       "2016-01-24   NaN     NaN      0  4.85     1.5       NaN             NaN   \n",
       "2016-01-24   NaN     NaN      0  5.25     2.0       NaN             NaN   \n",
       "2016-01-27   4.0     3.8      1  6.59     4.0       NaN             NaN   \n",
       "\n",
       "            Length  Circum  Volume  ...  Nopales  Lobster  Egg  Mushroom  \\\n",
       "Date                                ...                                    \n",
       "2016-01-18     NaN     NaN     NaN  ...        0        0    0         0   \n",
       "2016-01-24     NaN     NaN     NaN  ...        0        0    0         0   \n",
       "2016-01-24     NaN     NaN     NaN  ...        0        0    0         0   \n",
       "2016-01-24     NaN     NaN     NaN  ...        0        0    0         0   \n",
       "2016-01-27     NaN     NaN     NaN  ...        0        0    0         0   \n",
       "\n",
       "            Bacon  Sushi  Avocado  Corn  Zucchini Great  \n",
       "Date                                                     \n",
       "2016-01-18      0      0        0     0         0     0  \n",
       "2016-01-24      0      0        0     0         0     0  \n",
       "2016-01-24      0      0        0     0         0     0  \n",
       "2016-01-24      0      0        0     0         0     0  \n",
       "2016-01-27      0      0        0     0         0     1  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp               6\n",
      "Google            18\n",
      "Chips              2\n",
      "Cost              98\n",
      "Hunger            25\n",
      "Mass (g)          18\n",
      "Density (g/mL)    21\n",
      "Length            29\n",
      "Circum            30\n",
      "Volume            64\n",
      "Tortilla          18\n",
      "Temp              18\n",
      "Meat              23\n",
      "Fillings          22\n",
      "Meat:filling      25\n",
      "Uniformity        28\n",
      "Salsa             27\n",
      "Synergy           27\n",
      "Wrap              23\n",
      "Beef               2\n",
      "Pico               2\n",
      "Guac               2\n",
      "Cheese             2\n",
      "Fries              2\n",
      "Sour cream         2\n",
      "Pork               2\n",
      "Chicken            2\n",
      "Shrimp             2\n",
      "Fish               2\n",
      "Rice               2\n",
      "Beans              2\n",
      "Lettuce            2\n",
      "Tomato             2\n",
      "Bell peper         2\n",
      "Carrots            2\n",
      "Cabbage            2\n",
      "Sauce              2\n",
      "Salsa.1            2\n",
      "Cilantro           2\n",
      "Onion              2\n",
      "Taquito            2\n",
      "Pineapple          2\n",
      "Ham                2\n",
      "Chile relleno      2\n",
      "Nopales            2\n",
      "Lobster            2\n",
      "Egg                2\n",
      "Mushroom           2\n",
      "Bacon              2\n",
      "Sushi              2\n",
      "Avocado            2\n",
      "Corn               2\n",
      "Zucchini           2\n",
      "Great              2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
    "df = wrangle(filepath)\n",
    "# print(df.shape)\n",
    "# print(df.info())\n",
    "display(df.head())\n",
    "# print(df.tail())\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
    "\n",
    "```\n",
    "0      x\n",
    "1      x\n",
    "2    NaN\n",
    "3      x\n",
    "4      x\n",
    "Name: Beef, dtype: object\n",
    "```\n",
    "\n",
    "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct your exploratory data analysis here\n",
    "# And modify the `wrangle` function above.\n",
    "# print(df['Hunger'][0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
    "\n",
    "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
    "\n",
    "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
    "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
    "| Carne asada |       0        |     1     |    0     |      0       |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "\n",
    "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct your exploratory data analysis here\n",
    "# And modify the `wrangle` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Split Data\n",
    "\n",
    "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Great'\n",
    "y = df[target]\n",
    "X = df.drop(columns=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
    "\n",
    "- Your training set should include data from 2016 through 2017. \n",
    "- Your test set should include data from 2018 and later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = '2018-01-01'\n",
    "mask = X.index < cutoff\n",
    "\n",
    "X_train, y_train = X.loc[mask], y.loc[mask]\n",
    "X_test, y_test = X.loc[~mask], y.loc[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Establish Baseline\n",
    "\n",
    "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy Score: 0.5822454308093995\n"
     ]
    }
   ],
   "source": [
    "baseline_acc = y_train.value_counts(normalize=True).max()\n",
    "print('Baseline Accuracy Score:', baseline_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Build Model\n",
    "\n",
    "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
    "\n",
    "- a `OneHotEncoder` transformer for categorical features, \n",
    "- a `SimpleImputer` transformer to deal with missing values, \n",
    "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
    "- a `LogisticRegression` predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['Beef', 'Pico'], use_cat_names=True)),\n",
       "                ('simpleimputer', SimpleImputer()),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_logr = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "model_logr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Check Metrics\n",
    "\n",
    "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.9086161879895561\n",
      "Test MAE: 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "training_acc = model_logr.score(X_train, y_train)\n",
    "test_acc = model_logr.score(X_test, y_test)\n",
    "\n",
    "print('Training MAE:', training_acc)\n",
    "print('Test MAE:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Communicate Results\n",
    "\n",
    "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
    "\n",
    "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['onehotencoder', 'simpleimputer', 'standardscaler', 'logisticregression'])\n",
      "[[ 4.17475642e-01 -2.94012138e-01 -3.09464807e-02  2.58916608e-01\n",
      "   1.96711882e-01 -1.36449306e-01 -7.80940973e-02  1.58077883e-01\n",
      "   3.77443325e-03 -1.98337248e-02  3.74232366e-01  6.27738577e-01\n",
      "   1.59519052e+00  1.36953058e+00  1.21448845e+00  5.22643613e-03\n",
      "   3.77988601e-01  1.73293581e+00 -9.38994957e-03  1.02165048e-01\n",
      "   1.91229161e-01 -4.60528697e-01 -1.14057338e-01  1.44688468e-01\n",
      "  -6.52750525e-02  3.45832414e-01 -9.96612471e-02  2.14623117e-01\n",
      "   1.88697715e-01  1.68830798e-01 -3.06503346e-01 -1.11577540e-01\n",
      "  -1.11452318e-01  3.39081178e-01 -6.15745561e-01 -3.44374760e-01\n",
      "  -1.28447385e-01 -2.39264945e-01 -8.24563401e-03 -1.03325251e-03\n",
      "   2.61213575e-02  1.91627805e-01  1.91209394e-01  4.06013743e-02\n",
      "  -8.82623686e-02 -6.65156289e-01  3.84521074e-01  2.26051365e-02\n",
      "   1.83444521e-01 -8.36016110e-02 -1.01107500e-02 -3.02900357e-02\n",
      "   1.26261172e-01  2.36908387e-01 -1.50850847e-01 -3.46786733e-01\n",
      "   2.84874875e-01]]\n",
      "['Yelp', 'Google', 'Chips', 'Cost', 'Hunger', 'Mass (g)', 'Density (g/mL)', 'Length', 'Circum', 'Volume', 'Tortilla', 'Temp', 'Meat', 'Fillings', 'Meat:filling', 'Uniformity', 'Salsa', 'Synergy', 'Wrap', 'Beef_x', 'Beef_nan', 'Beef_X', 'Pico_x', 'Pico_nan', 'Pico_X', 'Guac', 'Cheese', 'Fries', 'Sour cream', 'Pork', 'Chicken', 'Shrimp', 'Fish', 'Rice', 'Beans', 'Lettuce', 'Tomato', 'Bell peper', 'Carrots', 'Cabbage', 'Sauce', 'Salsa.1', 'Cilantro', 'Onion', 'Taquito', 'Pineapple', 'Ham', 'Chile relleno', 'Nopales', 'Lobster', 'Egg', 'Mushroom', 'Bacon', 'Sushi', 'Avocado', 'Corn', 'Zucchini']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your horizontal barchart here.\n",
    "import matplotlib.pyplot as plt\n",
    "print(model_logr.named_steps.keys())\n",
    "coefficients = model_logr.named_steps['logisticregression'].coef_\n",
    "print(coefficients)\n",
    "feat_names = model_logr.named_steps['onehotencoder'].get_feature_names()\n",
    "print(feat_names)\n",
    "pd.Series(coefficients[0], index=feat_names).sort_values(key=abs).tail(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
    "\n",
    "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
    "\n",
    "- What data type do `predict` and `predict_proba` output?\n",
    "- What are the shapes of their different output?\n",
    "- What numerical values are in the output?\n",
    "- What do those numerical values represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
      " 1]\n",
      "[[5.82111725e-05 9.99941789e-01]\n",
      " [3.32880078e-03 9.96671199e-01]\n",
      " [5.20237381e-01 4.79762619e-01]\n",
      " [5.80637314e-05 9.99941936e-01]\n",
      " [9.97660496e-01 2.33950375e-03]\n",
      " [8.27516241e-01 1.72483759e-01]\n",
      " [3.40677451e-02 9.65932255e-01]\n",
      " [3.44348373e-04 9.99655652e-01]\n",
      " [1.58473073e-01 8.41526927e-01]\n",
      " [4.36394644e-02 9.56360536e-01]\n",
      " [3.45366069e-01 6.54633931e-01]\n",
      " [9.64471049e-01 3.55289507e-02]\n",
      " [4.90334373e-01 5.09665627e-01]\n",
      " [2.79065605e-01 7.20934395e-01]\n",
      " [1.07535523e-01 8.92464477e-01]\n",
      " [1.16792189e-01 8.83207811e-01]\n",
      " [2.26920876e-01 7.73079124e-01]\n",
      " [9.97134029e-01 2.86597137e-03]\n",
      " [9.98542252e-01 1.45774773e-03]\n",
      " [9.97158677e-01 2.84132287e-03]\n",
      " [9.85030753e-01 1.49692466e-02]\n",
      " [2.57723901e-02 9.74227610e-01]\n",
      " [8.01418223e-01 1.98581777e-01]\n",
      " [5.85343133e-01 4.14656867e-01]\n",
      " [1.56598996e-01 8.43401004e-01]\n",
      " [8.43515721e-01 1.56484279e-01]\n",
      " [9.11831560e-01 8.81684401e-02]\n",
      " [3.35610303e-05 9.99966439e-01]\n",
      " [9.08666309e-02 9.09133369e-01]\n",
      " [4.59191805e-03 9.95408082e-01]\n",
      " [9.22279663e-01 7.77203367e-02]\n",
      " [1.44684164e-01 8.55315836e-01]\n",
      " [8.46734070e-02 9.15326593e-01]\n",
      " [9.91254113e-01 8.74588728e-03]\n",
      " [1.65219125e-01 8.34780875e-01]\n",
      " [3.97647329e-01 6.02352671e-01]\n",
      " [1.23249245e-04 9.99876751e-01]\n",
      " [1.36823760e-02 9.86317624e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
    "print(model_logr.predict(X_test))\n",
    "print(model_logr.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your written answer here:**\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_214_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
